{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sbp95kS1TBWT"
      },
      "source": [
        "### **Product Information Chatbot using Langchain ü¶ú and Comet ‚òÑ :**:\n",
        "\n",
        "The Product Information Chatbot is a conversational interface designed to provide users with information about products. It informs on the details of various products, such as specifications, prices, and reviews."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3AwZ8VLsBVa"
      },
      "source": [
        "#### **Langchain**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7S9X0OKZFUq"
      },
      "source": [
        "#### Preliminary Setup\n",
        "Before starting make sure to register an account on comet. The account provides you with the API keys to access all the goodness comet has to serve."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nvb9RSrQYcjD"
      },
      "source": [
        "#### Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzenpxq--44m",
        "outputId": "5061719a-ef09-4fbc-b64b-72268cbed52c"
      },
      "outputs": [],
      "source": [
        "!pip install langchain openai comet_llm textstat tiktoken --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UHmmfPLpaeMa"
      },
      "outputs": [],
      "source": [
        "import comet_llm\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.prompts import (\n",
        "    ChatPromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        "    MessagesPlaceholder,\n",
        "    SystemMessagePromptTemplate,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOT7ws2v6N7V"
      },
      "source": [
        "#### Making a simple Chatbot from Prompt Engineering üß∞\n",
        "The section will include the how-to of creating a simple chatbot using langchain with simple prompt engineering. The chatbot will be able to remember the previous conversation and answer the queries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MN8WGzn7BVm"
      },
      "source": [
        "##### 1. Chat Models üó®\n",
        "Chat models are backed by large language models. LLMs are different such that they are completion models while chat models are specifically tuned for having conversation. In this, we will be using **GPT-3.5**. There are many chat models supported in langchain. The list can be found [here](https://python.langchain.com/docs/integrations/chat/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "set7LyA_61zI"
      },
      "outputs": [],
      "source": [
        "llm = ChatOpenAI(temperature = 0.4,\n",
        "                 openai_api_key=\"...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVyELyjjYh_o"
      },
      "source": [
        "##### 2. Prompt üßë\n",
        "\n",
        "Prompt engineering tailors chatbots from generalization to specialization. Langchian provides prompt templates that simplifies the creation of prompts by combining default messages, user input, chat history, and, optionally, additional context retrieved during the conversation.\n",
        "\n",
        "`ChatPromptTemplate` takes a list of `MessagePromptTemplate`. LangChain provides different types of MessagePromptTemplate.\n",
        "The most commonly used are `AIMessagePromptTemplate`, `SystemMessagePromptTemplate` and `HumanMessagePromptTemplate`, which create an AI message, system message and human message respectively.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F8XNoPE00SdX"
      },
      "outputs": [],
      "source": [
        "instructions = \"\"\"You are a friendly chatbot capable of answering questions related to products. User's can ask questions about its specifications,\n",
        "            prices and reviews. Be polite and redirect conversation specifically to product information when necessary.\"\"\"\n",
        "\n",
        "human = \"Chat history of the user: {chat_history}\\nNew human question: {input}\"\n",
        "\n",
        "prompt = ChatPromptTemplate(\n",
        "    messages=[\n",
        "        SystemMessagePromptTemplate.from_template(instructions),\n",
        "        # The `variable_name` here is what must align with memory\n",
        "        HumanMessagePromptTemplate.from_template(human), #User query will go here\n",
        "    ],\n",
        "\n",
        "    input_variables=['chat_history','input'],\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bf9Uhl6MfZHH"
      },
      "source": [
        "##### 3. Memory üìù\n",
        "Memory allows the conversation to be stored and adds it to the current query for reference of the previous conversation. There are many memories supported by Langchain. The most common ones are as follows:\n",
        "\n",
        "* `ConversationBufferMemory`: Simplest type of Memory\n",
        "\n",
        "* `ConversationBufferWindowMemory`: keeps a list of the interactions of the conversation over time.\n",
        "\n",
        "* `ConversationSummaryMemory`: Summary is created for the stored conversation. This condenses the information and reduces the chances of token limit problems.\n",
        "\n",
        "* `ConversationTokenBufferMemory`: Truncates the stored messages if the token limit is reached as given by the user.\n",
        "\n",
        "Our chatbot will keep the track of last 4 interactions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b0ls62TW_DwO"
      },
      "outputs": [],
      "source": [
        "memory = ConversationBufferWindowMemory(memory_key = \"chat_history\",k = 4) #this will keep track of prev conversation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vwzvcTpnSFj"
      },
      "source": [
        "#### 4. Chains:\n",
        "Chains is a simple concept of connecting different pieces like language model, prompt, memory etc into a single chain. There are two ways to achieve it. One is the legacy way through `LLMChain` while the other is LangChain Expression Language (LCEL). Here we"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UxyY2K5YXD85",
        "outputId": "c02478ed-54cd-46b2-e2aa-29cf0d2f2c01"
      },
      "outputs": [],
      "source": [
        "conversation = ConversationChain(llm = llm,prompt = prompt, memory = memory, verbose = True) # LLMChain\n",
        "conversation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "id": "UjOiNckhm9dM",
        "outputId": "9b9b413c-3a71-4cc5-f9bd-03ec1dc9249f"
      },
      "outputs": [],
      "source": [
        "conversation.run(\"Which phone is better Samsung or Apple?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "HFN6R0JNLb4z",
        "outputId": "d5f67fc9-4fcc-4eff-b524-ea861ba9943d"
      },
      "outputs": [],
      "source": [
        "conversation.run(\"Compare them with onePlus?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dal_NilPnAP"
      },
      "source": [
        "#### 5. Documents\n",
        "We have created the chatbot, Now we need to pass in document to the chatbot for lookup and answer the product related question including reviews, product specifications and prices. Let's import the dataset from data.world which inlcudes Amazon Products. Here's the [link](https://query.data.world/s/76kyenosebb7rtcruaarjgnfogum66?dws=00000) to the dataset.\n",
        "\n",
        "* Importing Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69F8x7FkMNOQ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('https://query.data.world/s/76kyenosebb7rtcruaarjgnfogum66?dws=00000')\n",
        "\n",
        "df.to_csv('product.csv', sep=',', index=False, encoding='utf-8')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 533
        },
        "id": "3C9ADjIybjmj",
        "outputId": "b31bd239-5521-4be6-9b71-796e92c48807"
      },
      "outputs": [],
      "source": [
        "df.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MmSGyLHCJnp"
      },
      "source": [
        "* Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "anS4ABzuURWr"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders.csv_loader import CSVLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "loader = CSVLoader(file_path='product.csv',csv_args={\n",
        "                'delimiter': ','})\n",
        "\n",
        "data = loader.load()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgjPJGYkB3EL"
      },
      "source": [
        "* Splitting Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aRG7kCIL5f2d"
      },
      "outputs": [],
      "source": [
        "Splitter = RecursiveCharacterTextSplitter(chunk_size = 1500, chunk_overlap = 150)\n",
        "splits = Splitter.create_documents([datum.page_content for datum in data])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCmBAsPyZI17"
      },
      "source": [
        "##### 6. Chat Retreival\n",
        "In order to chat with the documents or some other source of knowledge (in our case its the product CSV), RAG (Retrieval Augmented Generation) is used. RAG is a technique for augmenting LLM knowledge with additional, often private or real-time, data.\n",
        "\n",
        "LLMs can reason about wide-ranging topics, but their knowledge is limited to the public data up to a specific point in time that they were trained on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CVPVHCPKezpi"
      },
      "outputs": [],
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings(\n",
        "             openai_api_key=\"...\") # Api Key\n",
        "\n",
        "vectorstore = FAISS.from_documents(splits, embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWpTB0bnhnyK"
      },
      "source": [
        "* Retrieve:\n",
        "\n",
        "  LangChain establishes a Retriever interface that encapsulates an index capable of providing pertinent documents in response to a textual query. All retrievers uniformly implement the method¬†`get_relevant_documents()`¬†and its asynchronous counterpart,¬†`aget_relevant_documents()`.\n",
        "\n",
        "We also need to input the context to the prompt for the chain to work."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rlRYEbzUpVYS"
      },
      "outputs": [],
      "source": [
        "instructions = \"\"\"You are a friendly chatbot capable of answering questions related to products. User's can ask questions about its specifications,\n",
        "            prices and reviews. Be polite and redirect conversation specifically to product information when necessary.\"\"\"\n",
        "\n",
        "human = \"\"\"\n",
        "The context is provided as: {context}\n",
        "New human question: {question}\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate(\n",
        "    messages=[\n",
        "        SystemMessagePromptTemplate.from_template(instructions),\n",
        "        # The `variable_name` here is what must align with memory\n",
        "        HumanMessagePromptTemplate.from_template(human), #User query will go here\n",
        "    ],\n",
        "    input_variables=['context','question'],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kNBcPgDC1C1"
      },
      "source": [
        "* Chain:\n",
        "  \n",
        "  Langchain conversation Retrieval chain is very useful to cater memory, retriever and prompt altogether.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9GWHmm2LhV4y"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import ConversationalRetrievalChain\n",
        "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})\n",
        "qa = ConversationalRetrievalChain.from_llm(llm = llm,\n",
        "                                           memory=memory,\n",
        "                                           get_chat_history=lambda x : x,\n",
        "                                           retriever=retriever,\n",
        "                                           combine_docs_chain_kwargs={\"prompt\": prompt}\n",
        "                                           )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7XsqOQH8L8n",
        "outputId": "e1fbab50-fcb1-4890-a8da-3c45ff1a3cb0"
      },
      "outputs": [],
      "source": [
        "qa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WOyhxZDf32xh"
      },
      "outputs": [],
      "source": [
        "def predict(question):\n",
        "\n",
        "  ai_msg = qa({\"question\":question})['answer']\n",
        "  return ai_msg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "hFILa5DWSBLk",
        "outputId": "f127c6b5-9491-4d09-9b76-786d28b97813"
      },
      "outputs": [],
      "source": [
        "predict(\"Hi,my name is Bob?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "NOiIPPx98ANc",
        "outputId": "bb9f6b5f-08ba-4642-d263-2bf5e5463889"
      },
      "outputs": [],
      "source": [
        "predict(\"What can you do?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "4V44rFEmSskk",
        "outputId": "404f6230-44ba-4714-e800-1fe6953b91b3"
      },
      "outputs": [],
      "source": [
        "predict(\"What is the best product?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "JYatvWGF9N98",
        "outputId": "34d1e4a4-160f-4d48-ae82-06a9637831d4"
      },
      "outputs": [],
      "source": [
        "predict(\"What's the price kindle paperwhite?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XeGPHWAPr-6e"
      },
      "source": [
        "#### Comet\n",
        "When creating applications with LLMs, the majority of the time is spent on prompt engineering rather than training the models. This brings a new term in town **LLMOps**. Comet has a rich set of features for LLMOps namely:\n",
        "\n",
        "* LLM Projects: It is designed for analyzing prompts, responses and chaining.\n",
        "\n",
        "* LLM Panels: Visualizations compatible with Experiment Management can be employed to observe prompts and chains, particularly beneficial for projects involving both fine-tuning and prompt engineering use-cases.\n",
        "\n",
        "Initially, it is required to create an account on [**Comet**](https://www.comet.com/signup) and then get the access of the api key to access its features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B10Jx0RlpJN3",
        "outputId": "e0260e9b-9668-4cb3-ecc3-203ace74338b"
      },
      "outputs": [],
      "source": [
        "\n",
        "#COMET_WORKSPACE = \"COMET_WORKSPACE\"\n",
        "PROJECT_NAME = \"Product-Bot-v2\"\n",
        "# initialize comet\n",
        "comet_llm.init(project=PROJECT_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "imqlmVXg0o8w",
        "outputId": "76beb72a-2c80-4e30-f531-ad20999bf093"
      },
      "outputs": [],
      "source": [
        "comet_llm.is_ready()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0zyKmI9vl9S"
      },
      "source": [
        "#### How to monitor and track on comet\n",
        "Now in order to track the outputs of the conversation bot and check whether the prompt is good for your use case, comet has a simple function `log_prompt`:\n",
        "\n",
        "log_prompt takes in the prompt or user query, prompt_template takes in the template and output takes in the output from the model.\n",
        "metadata can take a dict of various properties for example token usage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1BBz0rImGomH",
        "outputId": "32bd7473-86ef-4c50-c0e6-7045449ebc8f"
      },
      "outputs": [],
      "source": [
        "queries = [\"Hi,my name is Bob?\",\"What can you do?\",\"What is the best product?\",\"What's the price kindle paperwhite?\"]\n",
        "expected_response = [\"Hello, how can you assist you.\",\n",
        "              \"As a chatbot, my capabilities include answering questions about product specifications, prices, and reviews. \\\n",
        "               I can provide information about various products and assist you in finding the information you need. \\\n",
        "               If you have any specific questions or need assistance with a particular product, feel free to ask.\",\n",
        "              \"Based on the information provided, it seems that there is no single clear winner among the tablets mentioned.\\\n",
        "               Each tablet has its own strengths and weaknesses. The Amazon HDX and Nexus are praised for their pricing, \\\n",
        "               while Apple and Google have more app choices. If you are heavily invested in the Apple ecosystem, \\\n",
        "               the iPad Mini might be a good choice. Ultimately, the best product will depend on your personal preferences and requirements.\",\n",
        "              \"The base model of the Kindle Paperwhite is priced at $99.\"]\n",
        "\n",
        "#for index, convo in enumerate(queries):\n",
        "    # log the few-shot predictions\n",
        "\n",
        "comet_llm.log_prompt(\n",
        "  prompt=queries[0],\n",
        "  prompt_template= instructions,\n",
        "  output = predict(queries[0]),\n",
        "  tags = [\"gpt-3.5-turbo\", \"prompt_1\"],\n",
        "  metadata = {\n",
        "    \"expected_answer\": expected_response[0]\n",
        "    },\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQNmg5mJisBN"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2rc2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
